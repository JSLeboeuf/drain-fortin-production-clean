name: CI/CD Pipeline - Drain Fortin Production

on:
  push:
    branches: [main, develop, 'feature/*', 'hotfix/*', 'release/*']
  pull_request:
    branches: [main, develop]
  schedule:
    - cron: '0 2 * * 1' # Weekly security scans on Monday 2AM
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      skip_tests:
        description: 'Skip tests (emergency only)'
        required: false
        default: false
        type: boolean

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

env:
  NODE_VERSION: '20'
  DENO_VERSION: '1.46'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  COVERAGE_THRESHOLD: '80'
  PERFORMANCE_THRESHOLD: '85'

jobs:
  # ==========================================
  # CODE QUALITY AND SECURITY
  # ==========================================
  code-quality:
    name: 'Code Quality & Security'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      actions: read
    outputs:
      lint-passed: ${{ steps.lint.outcome == 'success' }}
      type-check-passed: ${{ steps.typecheck.outcome == 'success' }}
      security-scan-passed: ${{ steps.security.outcome == 'success' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Setup Deno
        uses: denoland/setup-deno@v1
        with:
          deno-version: ${{ env.DENO_VERSION }}

      - name: Cache frontend dependencies
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci --prefer-offline --no-audit

      - name: Run ESLint with SARIF output
        id: lint
        run: |
          cd frontend
          npm run lint -- --format @microsoft/eslint-formatter-sarif --output-file eslint-results.sarif || true
          npm run lint

      - name: Upload ESLint results to GitHub
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: frontend/eslint-results.sarif
          wait-for-processing: true

      - name: Run TypeScript check
        id: typecheck
        run: |
          cd frontend
          npm run type-check || npx tsc --noEmit

      - name: Run Prettier check
        run: |
          cd frontend
          npx prettier --check "src/**/*.{ts,tsx,js,jsx,json,css,md}"

      - name: CodeQL Analysis
        uses: github/codeql-action/init@v3
        with:
          languages: javascript,typescript
          config-file: ./.github/codeql/codeql-config.yml

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

      - name: Dependency vulnerability scan
        id: security
        run: |
          cd frontend
          npm audit --audit-level=moderate
          npx audit-ci --moderate

      - name: License compliance check
        run: |
          cd frontend
          npx license-checker --onlyAllow 'MIT;Apache-2.0;BSD-2-Clause;BSD-3-Clause;ISC;0BSD;Unlicense;CC0-1.0' --excludePrivatePackages

      - name: Backend Deno lint and format check
        run: |
          cd backend
          deno lint --unstable
          deno fmt --check

      - name: Backend Deno type check
        run: |
          cd backend
          find supabase/functions -name "*.ts" -exec deno check {} \;

      - name: Secrets scanning
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: main
          head: HEAD
          extra_args: --debug --only-verified

      - name: SAST Scan with Semgrep
        uses: semgrep/semgrep-action@v1
        with:
          config: >-
            p/security-audit
            p/secrets
            p/owasp-top-ten
            p/typescript
            p/react
          generateSarif: "1"

      - name: Upload Semgrep results to GitHub
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: semgrep.sarif

      - name: OWASP ZAP Baseline Scan
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        uses: zaproxy/action-baseline@v0.12.0
        with:
          target: ${{ github.ref == 'refs/heads/main' && secrets.PRODUCTION_URL || secrets.STAGING_URL }}
          rules_file_name: '.zap/rules.tsv'
          cmd_options: '-a -d -T 60 -m 5'

      - name: Upload ZAP results
        if: always() && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch')
        uses: actions/upload-artifact@v4
        with:
          name: zap-report
          path: report_html.html

  # ==========================================
  # TESTING PIPELINE
  # ==========================================
  test-frontend:
    name: 'Frontend Tests'
    runs-on: ubuntu-latest
    needs: code-quality
    if: ${{ !inputs.skip_tests }}
    strategy:
      fail-fast: false
      matrix:
        node-version: [18, 20, 22]
    outputs:
      coverage-percentage: ${{ steps.coverage.outputs.percentage }}
      coverage-passed: ${{ steps.coverage.outputs.passed }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Cache node modules
        uses: actions/cache@v4
        with:
          path: frontend/node_modules
          key: ${{ runner.os }}-node-${{ matrix.node-version }}-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-${{ matrix.node-version }}-

      - name: Install dependencies
        run: |
          cd frontend
          npm ci --prefer-offline

      - name: Run unit tests with coverage
        run: |
          cd frontend
          npm run test:run -- --coverage --reporter=junit --reporter=json --outputFile=./coverage/coverage-summary.json

      - name: Check coverage threshold
        if: matrix.node-version == 20
        id: coverage
        run: |
          cd frontend
          COVERAGE=$(node -p "JSON.parse(require('fs').readFileSync('./coverage/coverage-summary.json')).total.lines.pct")
          echo "percentage=${COVERAGE}" >> $GITHUB_OUTPUT
          
          if (( $(echo "$COVERAGE >= ${{ env.COVERAGE_THRESHOLD }}" | bc -l) )); then
            echo "‚úÖ Coverage: ${COVERAGE}% (threshold: ${{ env.COVERAGE_THRESHOLD }}%)"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Coverage: ${COVERAGE}% (threshold: ${{ env.COVERAGE_THRESHOLD }}%)"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Generate coverage badges
        if: matrix.node-version == 20
        run: |
          cd frontend
          npx coverage-badges-cli --source coverage/coverage-summary.json --output coverage/badges

      - name: Run accessibility tests
        if: matrix.node-version == 20
        run: |
          cd frontend
          npm run test:a11y || true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-node-${{ matrix.node-version }}
          path: |
            frontend/coverage/
            frontend/junit.xml
            frontend/test-results.json

      - name: Upload coverage to Codecov
        if: matrix.node-version == 20
        uses: codecov/codecov-action@v4
        with:
          directory: ./frontend/coverage/
          fail_ci_if_error: true
          flags: frontend
          name: codecov-frontend

      - name: Comment coverage on PR
        if: matrix.node-version == 20 && github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          recreate: true
          message: |
            ## üìä Test Coverage Report
            
            **Coverage**: ${{ steps.coverage.outputs.percentage }}%
            **Threshold**: ${{ env.COVERAGE_THRESHOLD }}%
            **Status**: ${{ steps.coverage.outputs.passed == 'true' && '‚úÖ PASSED' || '‚ùå FAILED' }}
            
            [View detailed coverage report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

  test-backend:
    name: 'Backend Tests'
    runs-on: ubuntu-latest
    needs: code-quality
    if: ${{ !inputs.skip_tests }}
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Deno
        uses: denoland/setup-deno@v1
        with:
          deno-version: ${{ env.DENO_VERSION }}

      - name: Cache Deno modules
        uses: actions/cache@v4
        with:
          path: ~/.cache/deno
          key: ${{ runner.os }}-deno-${{ hashFiles('**/deps.ts', '**/import_map.json') }}
          restore-keys: |
            ${{ runner.os }}-deno-

      - name: Install Supabase CLI
        run: |
          npm install -g @supabase/cli@latest

      - name: Start Supabase local development
        run: |
          cd backend
          supabase start --db-url postgresql://postgres:postgres@localhost:5432/postgres

      - name: Run backend tests
        run: |
          cd backend
          deno test --allow-all --coverage=coverage supabase/functions/

      - name: Generate coverage report
        run: |
          cd backend
          deno coverage coverage --lcov --output=coverage.lcov

      - name: Upload backend coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./backend/coverage.lcov
          flags: backend
          name: codecov-backend

  test-e2e:
    name: 'E2E Tests'
    runs-on: ubuntu-latest
    needs: [code-quality, test-frontend]
    if: ${{ !inputs.skip_tests }}
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: postgres
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Setup Deno
        uses: denoland/setup-deno@v1
        with:
          deno-version: ${{ env.DENO_VERSION }}

      - name: Install dependencies
        run: |
          cd frontend
          npm ci --prefer-offline

      - name: Install Supabase CLI
        run: |
          npm install -g @supabase/cli@latest

      - name: Start Supabase local development
        run: |
          cd backend
          supabase start --db-url postgresql://postgres:postgres@localhost:5432/postgres

      - name: Install Playwright browsers
        run: |
          cd frontend
          npx playwright install --with-deps ${{ matrix.browser }}

      - name: Build frontend
        run: |
          cd frontend
          npm run build

      - name: Start preview server
        run: |
          cd frontend
          npm run preview &
          sleep 15
          curl -f http://localhost:5173 || exit 1

      - name: Run E2E tests
        run: |
          cd frontend
          npx playwright test --project=${{ matrix.browser }}

      - name: Upload E2E results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.browser }}
          path: |
            frontend/playwright-report/
            frontend/test-results/

      - name: Upload E2E videos
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: playwright-videos-${{ matrix.browser }}
          path: frontend/test-results/**/video.webm

  # ==========================================
  # BUILD AND CONTAINERIZATION
  # ==========================================
  build:
    name: 'Build & Container'
    runs-on: ubuntu-latest
    needs: [test-frontend, test-backend, test-e2e]
    if: success() || inputs.skip_tests
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-url: ${{ steps.build.outputs.image-url }}
      image-tag: ${{ steps.meta.outputs.tags }}
      sbom: ${{ steps.sbom.outputs.sbom }}
    permissions:
      contents: read
      packages: write
      security-events: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}

      - name: Build and push container image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64
          build-args: |
            NODE_ENV=production
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}
            VERSION=${{ github.ref_name }}

      - name: Generate SBOM
        id: sbom
        uses: anchore/sbom-action@v0
        with:
          image: ${{ steps.meta.outputs.tags }}
          format: spdx-json
          output-file: /tmp/sbom.spdx.json

      - name: Upload SBOM
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: /tmp/sbom.spdx.json

      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.meta.outputs.tags }}
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Scan image with Snyk
        continue-on-error: true
        uses: snyk/actions/docker@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          image: ${{ steps.meta.outputs.tags }}
          args: --severity-threshold=high --file=Dockerfile

      - name: Run container structure test
        run: |
          curl -LO https://storage.googleapis.com/container-structure-test/latest/container-structure-test-linux-amd64
          chmod +x container-structure-test-linux-amd64
          ./container-structure-test-linux-amd64 test --image ${{ steps.meta.outputs.tags }} --config .github/container-structure-test.yaml

      - name: Sign container image with Cosign
        if: github.event_name != 'pull_request'
        uses: sigstore/cosign-installer@v3
        with:
          cosign-release: 'v2.1.1'

      - name: Sign the published Docker image
        if: github.event_name != 'pull_request'
        env:
          COSIGN_EXPERIMENTAL: 1
        run: echo "${{ steps.meta.outputs.tags }}" | xargs -I {} cosign sign --yes {}@${{ steps.build.outputs.digest }}

  # ==========================================
  # INFRASTRUCTURE VALIDATION
  # ==========================================
  infrastructure:
    name: 'Infrastructure Validation'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform fmt check
        run: |
          if [ -d "infrastructure" ]; then
            terraform fmt -check -recursive infrastructure/
          fi

      - name: Validate infrastructure
        run: |
          if [ -d "infrastructure" ]; then
            cd infrastructure
            terraform init -backend=false
            terraform validate
          fi

  # ==========================================
  # DEPLOYMENT TO STAGING
  # ==========================================
  deploy-staging:
    name: 'Deploy to Staging'
    runs-on: ubuntu-latest
    needs: [build, infrastructure]
    if: github.ref == 'refs/heads/develop' || (github.event_name == 'workflow_dispatch' && inputs.environment == 'staging')
    environment: 
      name: staging
      url: ${{ steps.deploy.outputs.staging-url }}
    outputs:
      deployment-id: ${{ steps.deploy.outputs.deployment-id }}
      staging-url: ${{ steps.deploy.outputs.staging-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create deployment record
        id: deployment
        uses: chrnorm/deployment-action@v2
        with:
          token: ${{ github.token }}
          environment: staging
          description: 'Staging deployment for commit ${{ github.sha }}'

      - name: Deploy Frontend to Vercel (Staging)
        id: vercel-deploy
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./frontend
          scope: ${{ secrets.VERCEL_ORG_ID }}

      - name: Setup Supabase CLI
        run: |
          npm install -g @supabase/cli@latest

      - name: Deploy Supabase Functions (Staging)
        run: |
          cd backend
          supabase link --project-ref ${{ secrets.SUPABASE_PROJECT_REF_STAGING }}
          supabase db push
          supabase functions deploy --project-ref ${{ secrets.SUPABASE_PROJECT_REF_STAGING }}
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}

      - name: Wait for deployment to stabilize
        run: sleep 30

      - name: Run comprehensive smoke tests
        id: smoke-tests
        run: |
          # Frontend health check
          curl -f "${{ secrets.STAGING_URL }}/health" -w "%{http_code}" || exit 1
          
          # Backend health check  
          curl -f "${{ secrets.SUPABASE_URL }}/functions/v1/vapi-webhook" \
            -H "Content-Type: application/json" \
            -d '{"type":"health-check"}' -w "%{http_code}" || exit 1
          
          # Performance check
          response_time=$(curl -o /dev/null -s -w "%{time_total}" "${{ secrets.STAGING_URL }}")
          if (( $(echo "$response_time > 3" | bc -l) )); then
            echo "‚ùå Response time too slow: ${response_time}s"
            exit 1
          fi
          echo "‚úÖ Response time: ${response_time}s"

      - name: Run security baseline tests
        run: |
          # Test for security headers
          curl -I "${{ secrets.STAGING_URL }}" | grep -i "strict-transport-security" || exit 1
          curl -I "${{ secrets.STAGING_URL }}" | grep -i "x-content-type-options" || exit 1

      - name: Update deployment status (success)
        if: success()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: success
          environment-url: ${{ secrets.STAGING_URL }}

      - name: Update deployment status (failure)
        if: failure()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: failure

      - name: Set deployment outputs
        id: deploy
        run: |
          echo "deployment-id=${{ steps.deployment.outputs.deployment_id }}" >> $GITHUB_OUTPUT
          echo "staging-url=${{ secrets.STAGING_URL }}" >> $GITHUB_OUTPUT

      - name: Notify Slack - Success
        if: success()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-type: application/json' \
            -d '{
              "text": "üöÄ Staging Deployment Successful",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Staging Deployment Successful* ‚úÖ\n‚Ä¢ Commit: `${{ github.sha }}`\n‚Ä¢ Branch: `${{ github.ref_name }}`\n‚Ä¢ URL: ${{ secrets.STAGING_URL }}"
                  }
                }
              ]
            }'

      - name: Notify Slack - Failure
        if: failure()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-type: application/json' \
            -d '{
              "text": "‚ùå Staging Deployment Failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Staging Deployment Failed* ‚ùå\n‚Ä¢ Commit: `${{ github.sha }}`\n‚Ä¢ Branch: `${{ github.ref_name }}`\n‚Ä¢ <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Logs>"
                  }
                }
              ]
            }'

  # ==========================================
  # DEPLOYMENT TO PRODUCTION
  # ==========================================
  deploy-production:
    name: 'Deploy to Production'
    runs-on: ubuntu-latest
    needs: [build, infrastructure, deploy-staging]
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && inputs.environment == 'production')
    environment: 
      name: production
      url: ${{ steps.deploy.outputs.production-url }}
    outputs:
      deployment-id: ${{ steps.deploy.outputs.deployment-id }}
      production-url: ${{ steps.deploy.outputs.production-url }}
      rollback-url: ${{ steps.backup.outputs.rollback-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create deployment record
        id: deployment
        uses: chrnorm/deployment-action@v2
        with:
          token: ${{ github.token }}
          environment: production
          description: 'Production deployment for commit ${{ github.sha }}'

      - name: Backup current production state
        id: backup
        run: |
          # Store current deployment info for rollback
          echo "Creating rollback point..."
          CURRENT_DEPLOYMENT=$(curl -s -H "Authorization: Bearer ${{ secrets.VERCEL_TOKEN }}" \
            "https://api.vercel.com/v6/deployments?projectId=${{ secrets.VERCEL_PROJECT_ID }}&target=production&limit=1")
          ROLLBACK_URL=$(echo $CURRENT_DEPLOYMENT | jq -r '.deployments[0].url // ""')
          echo "rollback-url=${ROLLBACK_URL}" >> $GITHUB_OUTPUT
          echo "Rollback URL: ${ROLLBACK_URL}"

      - name: Deploy Frontend to Vercel (Production)
        id: vercel-deploy
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          vercel-args: '--prod'
          working-directory: ./frontend
          scope: ${{ secrets.VERCEL_ORG_ID }}

      - name: Setup Supabase CLI
        run: |
          npm install -g @supabase/cli@latest

      - name: Deploy Supabase Functions (Production)
        run: |
          cd backend
          supabase link --project-ref ${{ secrets.SUPABASE_PROJECT_REF }}
          supabase db push
          supabase functions deploy --project-ref ${{ secrets.SUPABASE_PROJECT_REF }}
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}

      - name: Update VAPI Assistant Configuration
        run: |
          curl -X PUT "https://api.vapi.ai/assistant/${{ secrets.VAPI_ASSISTANT_ID }}" \
            -H "Authorization: Bearer ${{ secrets.VAPI_API_KEY }}" \
            -H "Content-Type: application/json" \
            -d @config/vapi-assistant.json

      - name: Wait for deployment to stabilize
        run: sleep 45

      - name: Run comprehensive production health checks
        id: health-check
        run: |
          echo "Running production health checks..."
          
          # Frontend health check with retry
          for i in {1..5}; do
            if curl -f "${{ secrets.PRODUCTION_URL }}/health" -w "%{http_code}"; then
              echo "‚úÖ Frontend health check passed"
              break
            fi
            if [ $i -eq 5 ]; then
              echo "‚ùå Frontend health check failed after 5 attempts"
              exit 1
            fi
            sleep 10
          done
          
          # Backend health check
          for i in {1..5}; do
            if curl -f "${{ secrets.SUPABASE_URL }}/functions/v1/vapi-webhook" \
              -H "Content-Type: application/json" \
              -d '{"type":"health-check"}' -w "%{http_code}"; then
              echo "‚úÖ Backend health check passed"
              break
            fi
            if [ $i -eq 5 ]; then
              echo "‚ùå Backend health check failed after 5 attempts"
              exit 1
            fi
            sleep 10
          done
          
          # VAPI integration test
          if curl -f "https://api.vapi.ai/assistant/${{ secrets.VAPI_ASSISTANT_ID }}" \
            -H "Authorization: Bearer ${{ secrets.VAPI_API_KEY }}"; then
            echo "‚úÖ VAPI integration check passed"
          else
            echo "‚ùå VAPI integration check failed"
            exit 1
          fi

      - name: Run performance baseline tests
        id: performance
        run: |
          echo "Running performance tests..."
          
          # Install Lighthouse
          npm install -g lighthouse
          
          # Run Lighthouse audit
          lighthouse "${{ secrets.PRODUCTION_URL }}" \
            --chrome-flags="--headless --no-sandbox" \
            --output=json \
            --output-path=./lighthouse-report.json
          
          # Check performance score
          PERFORMANCE_SCORE=$(node -p "Math.round(JSON.parse(require('fs').readFileSync('./lighthouse-report.json')).categories.performance.score * 100)")
          echo "Performance score: ${PERFORMANCE_SCORE}%"
          
          if [ "$PERFORMANCE_SCORE" -lt "${{ env.PERFORMANCE_THRESHOLD }}" ]; then
            echo "‚ùå Performance score ${PERFORMANCE_SCORE}% is below threshold ${{ env.PERFORMANCE_THRESHOLD }}%"
            echo "performance-passed=false" >> $GITHUB_OUTPUT
          else
            echo "‚úÖ Performance score ${PERFORMANCE_SCORE}% meets threshold ${{ env.PERFORMANCE_THRESHOLD }}%"
            echo "performance-passed=true" >> $GITHUB_OUTPUT
          fi
          
          echo "performance-score=${PERFORMANCE_SCORE}" >> $GITHUB_OUTPUT

      - name: Run security validation tests
        run: |
          echo "Running security validation..."
          
          # Check security headers
          curl -I "${{ secrets.PRODUCTION_URL }}" | grep -i "strict-transport-security" || exit 1
          curl -I "${{ secrets.PRODUCTION_URL }}" | grep -i "x-content-type-options" || exit 1
          curl -I "${{ secrets.PRODUCTION_URL }}" | grep -i "x-frame-options" || exit 1
          curl -I "${{ secrets.PRODUCTION_URL }}" | grep -i "content-security-policy" || exit 1
          
          echo "‚úÖ Security headers validated"

      - name: Rollback on failure
        if: failure()
        run: |
          echo "üîÑ Initiating rollback due to deployment failure..."
          
          if [ -n "${{ steps.backup.outputs.rollback-url }}" ]; then
            echo "Rolling back to: ${{ steps.backup.outputs.rollback-url }}"
            # Trigger rollback via Vercel API or redeploy previous version
            curl -X POST "https://api.vercel.com/v13/deployments" \
              -H "Authorization: Bearer ${{ secrets.VERCEL_TOKEN }}" \
              -H "Content-Type: application/json" \
              -d '{
                "name": "rollback-deployment",
                "project": "${{ secrets.VERCEL_PROJECT_ID }}",
                "target": "production",
                "gitSource": {
                  "type": "github",
                  "ref": "main",
                  "sha": "${{ github.event.before }}"
                }
              }'
          else
            echo "‚ö†Ô∏è No rollback URL available"
          fi

      - name: Update deployment status (success)
        if: success()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: success
          environment-url: ${{ secrets.PRODUCTION_URL }}

      - name: Update deployment status (failure)
        if: failure()
        uses: chrnorm/deployment-status@v2
        with:
          token: ${{ github.token }}
          deployment-id: ${{ steps.deployment.outputs.deployment_id }}
          state: failure

      - name: Set deployment outputs
        id: deploy
        run: |
          echo "deployment-id=${{ steps.deployment.outputs.deployment_id }}" >> $GITHUB_OUTPUT
          echo "production-url=${{ secrets.PRODUCTION_URL }}" >> $GITHUB_OUTPUT

      - name: Notify Slack - Success
        if: success()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-type: application/json' \
            -d '{
              "text": "üöÄ Production Deployment Successful",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Production Deployment Successful* ‚úÖ\n‚Ä¢ Commit: `${{ github.sha }}`\n‚Ä¢ Performance: ${{ steps.performance.outputs.performance-score }}%\n‚Ä¢ URL: ${{ secrets.PRODUCTION_URL }}\n‚Ä¢ Rollback URL: ${{ steps.backup.outputs.rollback-url }}"
                  }
                }
              ]
            }'

      - name: Notify Slack - Failure
        if: failure()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-type: application/json' \
            -d '{
              "text": "‚ùå Production Deployment Failed - Rollback Initiated",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Production Deployment Failed* ‚ùå\n‚Ä¢ Commit: `${{ github.sha }}`\n‚Ä¢ Rollback initiated to: ${{ steps.backup.outputs.rollback-url }}\n‚Ä¢ <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Logs>"
                  }
                }
              ]
            }'

  # ==========================================
  # POST-DEPLOYMENT MONITORING
  # ==========================================
  post-deploy-monitoring:
    name: 'Post-deployment Monitoring'
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: success() && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup monitoring tools
        run: |
          npm install -g lighthouse artillery

      - name: Wait for deployment stabilization
        run: sleep 300

      - name: Run extended health checks
        id: health-monitoring
        run: |
          echo "üîç Running extended health monitoring..."
          
          # Extended frontend health monitoring
          for i in {1..10}; do
            RESPONSE_TIME=$(curl -o /dev/null -s -w "%{time_total}" "${{ secrets.PRODUCTION_URL }}/health")
            HTTP_CODE=$(curl -o /dev/null -s -w "%{http_code}" "${{ secrets.PRODUCTION_URL }}/health")
            echo "Health check $i: ${HTTP_CODE} (${RESPONSE_TIME}s)"
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "‚ùå Health check failed with code $HTTP_CODE"
              exit 1
            fi
            
            if (( $(echo "$RESPONSE_TIME > 5" | bc -l) )); then
              echo "‚ö†Ô∏è Slow response time: ${RESPONSE_TIME}s"
            fi
            
            sleep 30
          done
          
          echo "‚úÖ Extended health checks completed"

      - name: Synthetic transaction tests
        run: |
          echo "üß™ Running synthetic transaction tests..."
          
          # Test key user journeys
          curl -f "${{ secrets.PRODUCTION_URL }}/" -w "%{http_code}\\n"
          curl -f "${{ secrets.PRODUCTION_URL }}/about" -w "%{http_code}\\n"
          
          # Test API endpoints
          curl -f "${{ secrets.SUPABASE_URL }}/functions/v1/vapi-webhook" \
            -H "Content-Type: application/json" \
            -d '{"type":"ping"}' -w "%{http_code}\\n"

      - name: Load testing with Artillery
        run: |
          echo "üìä Running load tests..."
          
          cat > load-test.yml << EOF
          config:
            target: '${{ secrets.PRODUCTION_URL }}'
            phases:
              - duration: 120
                arrivalRate: 5
            defaults:
              headers:
                User-Agent: 'Artillery Load Test'
          scenarios:
            - name: "Homepage load test"
              requests:
                - get:
                    url: "/"
                - get:
                    url: "/health"
          EOF
          
          artillery run load-test.yml --output load-test-report.json
          artillery report load-test-report.json --output load-test-report.html
          
          # Extract key metrics
          P95_RESPONSE_TIME=$(cat load-test-report.json | jq '.aggregate.latency.p95')
          ERROR_RATE=$(cat load-test-report.json | jq '.aggregate.counters["http.codes.4xx"] // 0')
          
          echo "P95 Response Time: ${P95_RESPONSE_TIME}ms"
          echo "Error Rate: ${ERROR_RATE}"
          
          if (( $(echo "$P95_RESPONSE_TIME > 3000" | bc -l) )); then
            echo "‚ùå P95 response time too high: ${P95_RESPONSE_TIME}ms"
            exit 1
          fi

      - name: Availability monitoring
        run: |
          echo "üìà Setting up availability monitoring..."
          
          # Test from multiple locations (simulated)
          LOCATIONS=("us-east-1" "eu-west-1" "ap-southeast-1")
          
          for location in "${LOCATIONS[@]}"; do
            echo "Testing from $location..."
            RESPONSE_TIME=$(curl -o /dev/null -s -w "%{time_total}" "${{ secrets.PRODUCTION_URL }}")
            HTTP_CODE=$(curl -o /dev/null -s -w "%{http_code}" "${{ secrets.PRODUCTION_URL }}")
            
            echo "Location $location: ${HTTP_CODE} (${RESPONSE_TIME}s)"
            
            if [ "$HTTP_CODE" != "200" ]; then
              echo "‚ùå Failed from $location with code $HTTP_CODE"
            fi
          done

      - name: Performance regression testing
        run: |
          echo "‚ö° Running performance regression tests..."
          
          lighthouse "${{ secrets.PRODUCTION_URL }}" \
            --chrome-flags="--headless --no-sandbox" \
            --output=json \
            --output-path=./current-lighthouse-report.json \
            --quiet
          
          # Extract current scores
          CURRENT_PERFORMANCE=$(node -p "Math.round(JSON.parse(require('fs').readFileSync('./current-lighthouse-report.json')).categories.performance.score * 100)")
          CURRENT_ACCESSIBILITY=$(node -p "Math.round(JSON.parse(require('fs').readFileSync('./current-lighthouse-report.json')).categories.accessibility.score * 100)")
          CURRENT_SEO=$(node -p "Math.round(JSON.parse(require('fs').readFileSync('./current-lighthouse-report.json')).categories.seo.score * 100)")
          CURRENT_BEST_PRACTICES=$(node -p "Math.round(JSON.parse(require('fs').readFileSync('./current-lighthouse-report.json')).categories['best-practices'].score * 100)")
          
          echo "Performance: ${CURRENT_PERFORMANCE}%"
          echo "Accessibility: ${CURRENT_ACCESSIBILITY}%"
          echo "SEO: ${CURRENT_SEO}%"
          echo "Best Practices: ${CURRENT_BEST_PRACTICES}%"
          
          # Performance thresholds
          if [ "$CURRENT_PERFORMANCE" -lt "${{ env.PERFORMANCE_THRESHOLD }}" ]; then
            echo "‚ùå Performance regression detected: ${CURRENT_PERFORMANCE}% < ${{ env.PERFORMANCE_THRESHOLD }}%"
            exit 1
          fi
          
          if [ "$CURRENT_ACCESSIBILITY" -lt "90" ]; then
            echo "‚ùå Accessibility score too low: ${CURRENT_ACCESSIBILITY}%"
            exit 1
          fi

      - name: Generate monitoring report
        if: always()
        run: |
          cat > monitoring-report.md << EOF
          # Post-Deployment Monitoring Report
          
          **Deployment**: ${{ github.sha }}
          **Environment**: Production
          **Date**: $(date -u)
          
          ## Health Check Results
          - ‚úÖ Extended health monitoring passed
          - ‚úÖ Synthetic transaction tests passed
          
          ## Performance Metrics
          - Performance Score: ${CURRENT_PERFORMANCE:-"N/A"}%
          - Accessibility Score: ${CURRENT_ACCESSIBILITY:-"N/A"}%
          - SEO Score: ${CURRENT_SEO:-"N/A"}%
          - Best Practices Score: ${CURRENT_BEST_PRACTICES:-"N/A"}%
          
          ## Load Testing
          - P95 Response Time: ${P95_RESPONSE_TIME:-"N/A"}ms
          - Error Rate: ${ERROR_RATE:-"N/A"}
          
          EOF

      - name: Upload monitoring artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: monitoring-reports
          path: |
            load-test-report.html
            current-lighthouse-report.json
            monitoring-report.md

      - name: Update monitoring dashboard
        run: |
          echo "üìä Updating monitoring dashboard..."
          # This would typically integrate with your monitoring service
          # Example: Send metrics to Grafana, Datadog, etc.
          
      - name: Notify monitoring results
        if: always()
        run: |
          STATUS="${{ job.status }}"
          if [ "$STATUS" = "success" ]; then
            EMOJI="‚úÖ"
            MESSAGE="Post-deployment monitoring completed successfully"
          else
            EMOJI="‚ùå"
            MESSAGE="Post-deployment monitoring detected issues"
          fi
          
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
            -H 'Content-type: application/json' \
            -d "{
              \"text\": \"$EMOJI Post-deployment Monitoring\",
              \"blocks\": [
                {
                  \"type\": \"section\",
                  \"text\": {
                    \"type\": \"mrkdwn\",
                    \"text\": \"*$MESSAGE* $EMOJI\\n‚Ä¢ Performance: ${CURRENT_PERFORMANCE:-\"N/A\"}%\\n‚Ä¢ Load Test P95: ${P95_RESPONSE_TIME:-\"N/A\"}ms\\n‚Ä¢ Status: $STATUS\"
                  }
                }
              ]
            }"